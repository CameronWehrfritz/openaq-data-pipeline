{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b0d1b6",
   "metadata": {},
   "source": [
    "# Pull and Push OpenAQ data\n",
    "\n",
    "Pull air quality measurements from OpenAQ and push to BigQuery.\n",
    "\n",
    "-  Pulls latest hourly data for a list of predetermined sensors.\n",
    "\n",
    "-  Handles logging & job tracking.\n",
    "\n",
    "-  Writes directly from Pandas DataFrame to BigQuery.\n",
    "\n",
    "-  Finalizes job tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00309d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup & Config ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# --- OpenAQ API config ---\n",
    "OPENAQ_API_BASE = \"https://api.openaq.org/v3\"\n",
    "SENSOR_IDS = [1234, 5678, ...]  # <-- replace with your 59 sensor IDs\n",
    "\n",
    "# --- Logging config ---\n",
    "LOGS_DIR = \"logs\"\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "log_filename = os.path.join(LOGS_DIR, f\"job_log_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6cab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Credentials Path: C:\\\\Users\\\\camer\\\\.gcp_keys\\\\openaq_data_loader.json\n"
     ]
    }
   ],
   "source": [
    "# --- BigQuery constants and connect to client ---\n",
    "\n",
    "# BQ Project\n",
    "PROJECT_ID = \"openaq-data-pipeline-468404\"\n",
    "\n",
    "# BQ Data storage\n",
    "DATASET_ID_DATA = \"openaq_pm25\"\n",
    "TABLE_ID_DATA = \"pm25_hourly\"\n",
    "\n",
    "# BQ Job tracking\n",
    "DATASET_ID_JOBS = \"openaq_jobs\"\n",
    "TABLE_ID_JOBS = \"job_tracking\"\n",
    "\n",
    "# Load environment variables (e.g., GOOGLE_APPLICATION_CREDENTIALS)\n",
    "load_dotenv()\n",
    "\n",
    "# Confirm key path\n",
    "print(\"Google Credentials Path:\", os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "\n",
    "# Connect to BigQuery project\n",
    "bq_client = bigquery.Client(project=BQ_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab7926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table job_tracking...\n",
      "✅ Job tracking started: job_20250808_221854\n"
     ]
    }
   ],
   "source": [
    "# --- Job config ---\n",
    "\n",
    "# Make UTC-aware timestamps\n",
    "JOB_START_TIME = datetime.now(timezone.utc)\n",
    "\n",
    "# Create dataset if not exists\n",
    "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID_JOBS}\")\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "except Exception:\n",
    "    print(f\"Creating dataset {DATASET_ID_JOBS}...\")\n",
    "    bq_client.create_dataset(dataset_ref)\n",
    "\n",
    "# Create table if not exists\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"job_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"job_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"end_time\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"status\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"notes\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"error_message\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"rows_inserted\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"source_start_time\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"source_end_time\", \"TIMESTAMP\")\n",
    "]\n",
    "\n",
    "table_ref = f\"{PROJECT_ID}.{DATASET_ID_JOBS}.{TABLE_ID_JOBS}\"\n",
    "try:\n",
    "    bq_client.get_table(table_ref)\n",
    "except Exception:\n",
    "    print(f\"Creating table {TABLE_ID_JOBS}...\", flush=True)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    bq_client.create_table(table)\n",
    "\n",
    "# Generate a unique job_id\n",
    "job_id = f\"job_{JOB_START_TIME.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Insert start record\n",
    "rows_to_insert = [{\n",
    "    \"job_id\": job_id,\n",
    "    \"job_name\": \"OpenAQ_PM25_Pipeline\",\n",
    "    \"start_time\": JOB_START_TIME.isoformat(),\n",
    "    \"status\": \"RUNNING\",\n",
    "    \"notes\": \"Job started successfully.\",\n",
    "    \"error_message\": None,\n",
    "    \"rows_inserted\": None,\n",
    "    \"source_start_time\": None,\n",
    "    \"source_end_time\": None\n",
    "}]\n",
    "\n",
    "errors = bq_client.insert_rows_json(table_ref, rows_to_insert)\n",
    "\n",
    "if errors:\n",
    "    print(\"⚠️ Errors inserting job start record:\", errors, flush=True)\n",
    "else:\n",
    "    print(f\"✅ Job tracking started: {job_id}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd553ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                job_id              job_name                       start_time  \\\n",
      "0  job_20250808_221854  OpenAQ_PM25_Pipeline 2025-08-08 22:18:54.987150+00:00   \n",
      "\n",
      "  end_time   status                      notes error_message rows_inserted  \\\n",
      "0     None  RUNNING  Job started successfully.          None          None   \n",
      "\n",
      "  source_start_time source_end_time  \n",
      "0              None            None  \n"
     ]
    }
   ],
   "source": [
    "# quickly peek at the contents of your BigQuery job tracking table\n",
    "\n",
    "# query job table\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{DATASET_ID_JOBS}.{TABLE_ID_JOBS}`\n",
    "ORDER BY start_time DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "query_job = bq_client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "# Convert list of Row objects to list of dicts\n",
    "rows_as_dicts = [dict(results.items()) for results in results]\n",
    "\n",
    "# Create DataFrame\n",
    "df_jobs = pd.DataFrame(rows_as_dicts)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90abbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete your existing BigQuery table so you can recreate it with the updated schema\n",
    "\n",
    "# table_ref = f\"{PROJECT_ID}.{DATASET_ID_JOBS}.{TABLE_ID_JOBS}\"\n",
    "\n",
    "# try:\n",
    "#     bq_client.delete_table(table_ref)\n",
    "#     print(f\"Deleted table {table_ref}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting table: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openaq-env)",
   "language": "python",
   "name": "openaq-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
