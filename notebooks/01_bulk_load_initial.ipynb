{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a58619",
   "metadata": {},
   "source": [
    "# Set Up and Load OpenAQ data into BigQuery\n",
    "\n",
    "-  Reads local CSV (bulk OpenAQ dataset).\n",
    "\n",
    "-  Creates & populates the BigQuery table from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e15d3c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Credentials Path: C:\\\\Users\\\\camer\\\\.gcp_keys\\\\openaq_data_loader.json\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ 1. Setup environment & imports\n",
    "\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (e.g., GOOGLE_APPLICATION_CREDENTIALS)\n",
    "load_dotenv()\n",
    "\n",
    "# Confirm key path\n",
    "print(\"Google Credentials Path:\", os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125747d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery Dataset: openaq_ca\n",
      "BigQuery Table: pm25_hourly_ca\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ 2. Define your dataset and table names\n",
    "\n",
    "PROJECT_ID = client.project  # or hardcode your project id here\n",
    "DATASET_ID = \"openaq_ca\"\n",
    "TABLE_ID = \"pm25_hourly_ca\"\n",
    "FULL_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "\n",
    "print(f\"BigQuery Dataset: {DATASET_ID}\")\n",
    "print(f\"BigQuery Table: {TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f640f371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset openaq_ca\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ 3. Create the dataset if it doesnâ€™t exist\n",
    "\n",
    "def create_dataset(dataset_id):\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    try:\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "        print(f\"Dataset {dataset_id} already exists.\")\n",
    "    except Exception:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"US\"  # or your preferred region\n",
    "        dataset = client.create_dataset(dataset)\n",
    "        print(f\"Created dataset {dataset_id}\")\n",
    "\n",
    "create_dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276e71b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 851736 rows into openaq_ca.pm25_hourly_ca.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ 4. Load local CSV data to BigQuery table (replace path as needed)\n",
    "\n",
    "CSV_PATH = \"../data/openaq_hourly_ca_20160101_to_20250731.csv\"\n",
    "\n",
    "def load_csv_to_bq(csv_path, dataset_id, table_id):\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,       # Auto-detect schema from data\n",
    "        write_disposition=\"WRITE_TRUNCATE\",  # Overwrite table if exists\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1   # Skip CSV header\n",
    "    )\n",
    "\n",
    "    with open(csv_path, \"rb\") as source_file:\n",
    "        load_job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            job_config=job_config,\n",
    "        )\n",
    "\n",
    "    load_job.result()  # Wait for job to complete\n",
    "    print(f\"Loaded {load_job.output_rows} rows into {dataset_id}.{table_id}.\")\n",
    "\n",
    "load_csv_to_bq(CSV_PATH, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25fe82f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 sensors by record count:\n",
      "Sensor ID: 1561, Records: 18986\n",
      "Sensor ID: 1646, Records: 18622\n",
      "Sensor ID: 1618, Records: 18232\n",
      "Sensor ID: 1591, Records: 18130\n",
      "Sensor ID: 1595, Records: 18000\n",
      "Sensor ID: 1502, Records: 18000\n",
      "Sensor ID: 1624, Records: 18000\n",
      "Sensor ID: 1630, Records: 18000\n",
      "Sensor ID: 1639, Records: 18000\n",
      "Sensor ID: 1598, Records: 18000\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ 5. Verify loaded data with a simple query\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT sensor_id, COUNT(*) as count_records\n",
    "FROM `{FULL_TABLE_ID}`\n",
    "GROUP BY sensor_id\n",
    "ORDER BY count_records DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "print(\"Top 10 sensors by record count:\")\n",
    "for row in results:\n",
    "    print(f\"Sensor ID: {row.sensor_id}, Records: {row.count_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13a5920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row((350, 9.0, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((401, 2.3, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((537, 2.1, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((3050, 0.5, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((1556, 7.0, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((1469, 3.9, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((1502, 10.0, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((1554, 6.0, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((1561, 9.0, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n",
      "Row((1582, 14.0, 'Âµg/mÂ³', 'pm25', datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 19, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), datetime.datetime(2016, 3, 6, 20, 0, tzinfo=datetime.timezone.utc), 100.0), {'sensor_id': 0, 'value': 1, 'units': 2, 'pollutant': 3, 'datetime_from_utc': 4, 'datetime_from_local': 5, 'datetime_to_utc': 6, 'datetime_to_local': 7, 'coverage_pct': 8})\n"
     ]
    }
   ],
   "source": [
    "# show table\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{FULL_TABLE_ID}`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dbaee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camer\\professional\\data science\\projects\\openaq_pipeline_project\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1965: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensor_id  value  units pollutant         datetime_from_utc  \\\n",
      "0        350    9.0  Âµg/mÂ³      pm25 2016-03-06 19:00:00+00:00   \n",
      "1        401    2.3  Âµg/mÂ³      pm25 2016-03-06 19:00:00+00:00   \n",
      "2        537    2.1  Âµg/mÂ³      pm25 2016-03-06 19:00:00+00:00   \n",
      "3       3050    0.5  Âµg/mÂ³      pm25 2016-03-06 19:00:00+00:00   \n",
      "4       1556    7.0  Âµg/mÂ³      pm25 2016-03-06 19:00:00+00:00   \n",
      "\n",
      "        datetime_from_local           datetime_to_utc  \\\n",
      "0 2016-03-06 19:00:00+00:00 2016-03-06 20:00:00+00:00   \n",
      "1 2016-03-06 19:00:00+00:00 2016-03-06 20:00:00+00:00   \n",
      "2 2016-03-06 19:00:00+00:00 2016-03-06 20:00:00+00:00   \n",
      "3 2016-03-06 19:00:00+00:00 2016-03-06 20:00:00+00:00   \n",
      "4 2016-03-06 19:00:00+00:00 2016-03-06 20:00:00+00:00   \n",
      "\n",
      "          datetime_to_local  coverage_pct  \n",
      "0 2016-03-06 20:00:00+00:00         100.0  \n",
      "1 2016-03-06 20:00:00+00:00         100.0  \n",
      "2 2016-03-06 20:00:00+00:00         100.0  \n",
      "3 2016-03-06 20:00:00+00:00         100.0  \n",
      "4 2016-03-06 20:00:00+00:00         100.0  \n"
     ]
    }
   ],
   "source": [
    "df = query_job.to_dataframe()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa79470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table openaq-data-pipeline-468404.openaq_ca.pm25_hourly_ca truncated successfully.\n"
     ]
    }
   ],
   "source": [
    "# # Clear the entire table\n",
    "\n",
    "# # SQL to truncate the table\n",
    "# truncate_sql = f\"TRUNCATE TABLE `{FULL_TABLE_ID}`\"\n",
    "\n",
    "# # Run the query\n",
    "# query_job = client.query(truncate_sql)\n",
    "# query_job.result()  # Waits for the query to finish\n",
    "\n",
    "# print(f\"Table {FULL_TABLE_ID} truncated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openaq-env)",
   "language": "python",
   "name": "openaq-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
